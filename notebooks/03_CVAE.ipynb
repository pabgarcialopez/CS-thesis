{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run init_notebook.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import time\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from src.dataset import NSynth   \n",
    "from src.models import CVAE\n",
    "from src.utils.models import adjust_shape\n",
    "from src.utils.logger import save_training_results\n",
    "from src.utils.models import compute_magnitude_and_phase\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = r\"C:\\Users\\Articuno\\Desktop\\TFG-info\\data\\models\\cvae.pth\"\n",
    "\n",
    "# STFT transform\n",
    "sample_rate = 16000\n",
    "n_fft = 800\n",
    "hop_length = 100\n",
    "win_length = n_fft  # Same as n_fft\n",
    "\n",
    "# Apply the correct transform for magnitude and phase (onesided=False to handle complex spectrogram)\n",
    "stft_transform = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    power=None,  # Keep as complex spectrogram (magnitude and phase)\n",
    "    onesided=False,  # Make sure we keep the full spectrum (complex-valued)\n",
    "    center=False\n",
    ").to(device)\n",
    "\n",
    "# Define the inverse STFT function (onesided=False)\n",
    "istft_transform = torchaudio.transforms.InverseSpectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    onesided=False  # Make sure we reconstruct using the full complex spectrogram\n",
    ").to(device)\n",
    "\n",
    "# Datasets and DataLoaders, training parameters.\n",
    "train_dataset = NSynth(partition='training')\n",
    "valid_dataset = NSynth(partition='validation')\n",
    "test_dataset  = NSynth(partition='testing')\n",
    "\n",
    "batch_size = 64\n",
    "training_subset_size = len(train_dataset)\n",
    "train_dataset = Subset(train_dataset, list(range(training_subset_size)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "input_height = 800 \n",
    "input_width  = 633\n",
    "latent_dim   = 256\n",
    "learning_rate = 1.5e-3\n",
    "\n",
    "model = CVAE((input_height, input_width), latent_dim, 11).to(device)\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}. Unable to load the model.\")\n",
    "else:\n",
    "    print(\"No saved model found, starting training from scratch.\")\n",
    "\n",
    "    # Model, Optimizer, and Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 30\n",
    "    log_interval = 10\n",
    "    avg_epoch_time = 0.0\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Learning rate scheduler to reduce learning rate based on validation loss\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    print(f\"Starting training on {device}...\")\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "\n",
    "        model.train()\n",
    "        start_epoch_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Training Loop\n",
    "        for i, (waveform, _, _, metadata) in enumerate(train_loader):\n",
    "            # waveform has shape [batch_size, 1, time_steps]\n",
    "\n",
    "            waveform = waveform.to(device)\n",
    "\n",
    "            # Apply STFT transformation\n",
    "            stft_spec = stft_transform(waveform)  # [batch_size, 1, freq_bins, time_frames]\n",
    "\n",
    "            # Extract magnitude and phase\n",
    "            magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "\n",
    "            input = torch.cat([magnitude, phase], dim=1).to(device)  # [batch_size, 2, freq_bins, time_frames]\n",
    "            labels = torch.stack([meta[\"one_hot_instrument\"] for meta in metadata]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, mu, log_var = model(input, labels)  # shape [batch_size, 2, freq_bins, time_frames]\n",
    "\n",
    "            loss = model.loss_function(input, output, mu, log_var, input.shape[0])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "        # Compute average epoch loss for training\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (waveform, _, _, metadata) in enumerate(valid_loader):\n",
    "                waveform = waveform.to(device)\n",
    "                stft_spec = stft_transform(waveform)\n",
    "                \n",
    "                # Extract magnitude and phase\n",
    "                magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "                input = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "                labels = torch.stack([meta[\"one_hot_instrument\"] for meta in metadata]).to(device)\n",
    "\n",
    "                output, mu, log_var = model(input, labels)\n",
    "                loss = model.loss_function(input, output, mu, log_var, input.shape[0])\n",
    "                valid_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "        # Compute average validation loss\n",
    "        valid_loss /= len(valid_loader.dataset)        \n",
    "        \n",
    "        # Step the scheduler with validation loss\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        epoch_time = time.time() - start_epoch_time\n",
    "        avg_epoch_time += epoch_time\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, train_loss={train_loss}, valid_loss={valid_loss}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    save_training_results({\n",
    "        \"train_losses\": training_losses,\n",
    "        \"valid_loss\": validation_losses,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"avg_epoch_time\": avg_epoch_time / num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"training_subset_size\": training_subset_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"n_fft\": n_fft,\n",
    "        \"hop_length\": hop_length,\n",
    "        \"input_height\": input_height,\n",
    "        \"input_width\": input_width,\n",
    "        \"latent_dim\": latent_dim,\n",
    "    }, \"cvae.json\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, training_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs, validation_losses, label='Validation Loss', marker='x')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dataset import INSTRUMENT_ID_2_STR\n",
    "\n",
    "# Posterior Sampling (conditioned on instrument)\n",
    "model.eval()\n",
    "num_samples = 3  # Number of samples per instrument\n",
    "\n",
    "print(\"Posterior sampling (conditioned on instrument):\")\n",
    "print(\"---------------------------------------------\\n\")\n",
    "\n",
    "# Loop over all instruments\n",
    "for instrument_id in range(11):  # 11 instruments in total\n",
    "    print(f\"\\nGenerating samples for instrument: {INSTRUMENT_ID_2_STR[instrument_id]}\")\n",
    "\n",
    "    # Get a batch from the train_loader (DataLoader is not subscriptable)\n",
    "    batch = next(iter(train_loader))\n",
    "    waveform, _, _, metadata = batch\n",
    "\n",
    "    # Select the first num_samples from the batch\n",
    "    waveform = waveform[:num_samples]\n",
    "    waveform = waveform.to(device)\n",
    "    stft_spec = stft_transform(waveform)\n",
    "\n",
    "    magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "    input_tensor = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "\n",
    "    # Get the one-hot encoded label for the desired instrument (e.g., guitar)\n",
    "    labels = torch.stack([torch.tensor(meta[\"one_hot_instrument\"], dtype=torch.float) for meta in metadata]).to(device)\n",
    "\n",
    "    # Select the one-hot label for the specific instrument_id (e.g., guitar)\n",
    "    selected_label = labels[:num_samples, instrument_id].unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _, _ = model(input_tensor, selected_label)\n",
    "\n",
    "        recon_magnitude = output[:, 0, :, :]    # [batch, freq_bins, time_frames]\n",
    "        recon_phase = output[:, 1, :, :]        # [batch, freq_bins, time_frames]\n",
    "\n",
    "        recon_complex = recon_magnitude * torch.exp(1j * recon_phase)\n",
    "        reconstructed_waveform = istft_transform(recon_complex)  # [batch, time_steps]\n",
    "\n",
    "    # Play the generated samples\n",
    "    for i in range(reconstructed_waveform.shape[0]):\n",
    "        print(f\"\\n=== Generated Sample {i+1} for {INSTRUMENT_ID_2_STR[instrument_id]} ===\")\n",
    "        display(Audio(reconstructed_waveform[i].squeeze().cpu().numpy(), rate=sample_rate))\n",
    "\n",
    "\n",
    "\n",
    "# Prior Sampling (conditioned on instrument)\n",
    "model.eval()\n",
    "num_samples = 3  # Number of samples per instrument\n",
    "\n",
    "print(\"Prior sampling (conditioned on instrument):\")\n",
    "print(\"----------------------------------------\\n\")\n",
    "\n",
    "# Loop over all instruments\n",
    "for instrument_id in range(11):  # 11 instruments in total\n",
    "    print(f\"\\nGenerating samples for instrument: {INSTRUMENT_ID_2_STR[instrument_id]}\")\n",
    "\n",
    "    # Get the one-hot encoded label for the desired instrument (e.g., guitar)\n",
    "    instrument_label = torch.zeros(num_samples, 11).to(device)  # Assuming there are 11 instrument classes\n",
    "    instrument_label[:, instrument_id] = 1  # Set the label for the specific instrument (e.g., guitar)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Sample from N(0, 1) for the latent space\n",
    "        z = torch.distributions.Normal(0, 1).sample(sample_shape=(num_samples, latent_dim)).to(device)\n",
    "\n",
    "        # Pass the latent vectors and instrument label through the decoder to generate audio\n",
    "        generated_output = model.decoder(z, instrument_label)\n",
    "        generated_output = adjust_shape(generated_output, (input_height, input_width))\n",
    "\n",
    "        # The generated output is in two channels (magnitude and phase), so split them\n",
    "        gen_magnitude = generated_output[:, 0, :, :]\n",
    "        gen_phase = generated_output[:, 1, :, :]\n",
    "\n",
    "        # Recombine into a complex spectrogram\n",
    "        gen_complex = gen_magnitude * torch.exp(1j * gen_phase)\n",
    "        generated_waveforms = istft_transform(gen_complex)  # [num_samples, time_steps]\n",
    "\n",
    "    # Play the generated samples\n",
    "    for i in range(generated_waveforms.shape[0]):\n",
    "        print(f\"\\n=== Generated Sample {i+1} for {INSTRUMENT_ID_2_STR[instrument_id]} ===\")\n",
    "        display(Audio(generated_waveforms[i].squeeze().cpu().numpy(), rate=sample_rate))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
