{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: Encoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(2, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=512000, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=512000, out_features=128, bias=True)\n",
      ")\n",
      "Decoder: Decoder(\n",
      "  (fc): Linear(in_features=128, out_features=512000, bias=True)\n",
      "  (unflatten): Unflatten(dim=1, unflattened_size=(64, 100, 80))\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "No saved model found, starting training from scratch.\n",
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata is:  {'one_hot_instrument': tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [42:51<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m magnitude, phase \u001b[38;5;241m=\u001b[39m compute_magnitude_and_phase(stft_spec)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([magnitude, phase], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# [batch_size, 2, freq_bins, time_frames]\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Each meta is a one-hot encoded tensor\u001b[39;00m\n\u001b[0;32m    116\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    117\u001b[0m output, mu, log_var \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m, labels)  \u001b[38;5;66;03m# shape [batch_size, 2, freq_bins, time_frames]\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got str"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%run init_notebook.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import time\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from src.dataset import NSynth   \n",
    "from src.models import CVAE\n",
    "from src.utils.models import adjust_shape\n",
    "from src.utils.logger import save_training_results\n",
    "from src.utils.models import compute_magnitude_and_phase\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = r\"C:\\Users\\Articuno\\Desktop\\TFG-info\\data\\models\\cvae.pth\"\n",
    "\n",
    "# STFT transform\n",
    "sample_rate = 16000\n",
    "n_fft = 800\n",
    "hop_length = 100\n",
    "win_length = n_fft  # Same as n_fft\n",
    "\n",
    "# Apply the correct transform for magnitude and phase (onesided=False to handle complex spectrogram)\n",
    "stft_transform = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    power=None,  # Keep as complex spectrogram (magnitude and phase)\n",
    "    onesided=False,  # Make sure we keep the full spectrum (complex-valued)\n",
    "    center=False\n",
    ").to(device)\n",
    "\n",
    "# Define the inverse STFT function (onesided=False)\n",
    "istft_transform = torchaudio.transforms.InverseSpectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    onesided=False  # Make sure we reconstruct using the full complex spectrogram\n",
    ").to(device)\n",
    "\n",
    "# Datasets and DataLoaders, training parameters.\n",
    "train_dataset = NSynth(partition='training')\n",
    "valid_dataset = NSynth(partition='validation')\n",
    "test_dataset  = NSynth(partition='testing')\n",
    "\n",
    "batch_size = 64\n",
    "training_subset_size = len(train_dataset)\n",
    "train_dataset = Subset(train_dataset, list(range(training_subset_size)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "input_height = 800 \n",
    "input_width  = 633\n",
    "latent_dim   = 128\n",
    "learning_rate = 1e-4\n",
    "\n",
    "NUM_INSTRUMENT_CLASSES = 11\n",
    "\n",
    "model = CVAE((input_height, input_width), latent_dim, NUM_INSTRUMENT_CLASSES).to(device)\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}. Unable to load the model.\")\n",
    "else:\n",
    "    print(\"No saved model found, starting training from scratch.\")\n",
    "\n",
    "    # Model, Optimizer, and Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 30\n",
    "    log_interval = 10\n",
    "    avg_epoch_time = 0.0\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Learning rate scheduler to reduce learning rate based on validation loss\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    print(f\"Starting training on {device}...\")\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "\n",
    "        model.train()\n",
    "        start_epoch_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Training Loop\n",
    "        for i, (waveform, _, _, metadata) in enumerate(train_loader):\n",
    "            # waveform has shape [batch_size, 1, time_steps]\n",
    "\n",
    "            print(\"Metadata is: \" , metadata)\n",
    "            exit(0)\n",
    "\n",
    "            waveform = waveform.to(device)\n",
    "\n",
    "            # Apply STFT transformation\n",
    "            stft_spec = stft_transform(waveform)  # [batch_size, 1, freq_bins, time_frames]\n",
    "\n",
    "            # Extract magnitude and phase\n",
    "            magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "\n",
    "            input = torch.cat([magnitude, phase], dim=1).to(device)  # [batch_size, 2, freq_bins, time_frames]\n",
    "            labels = torch.stack([meta for meta in metadata]).to(device) # Each meta is a one-hot encoded tensor\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, mu, log_var = model(input, labels)  # shape [batch_size, 2, freq_bins, time_frames]\n",
    "\n",
    "            loss = model.loss_function(input, output, mu, log_var, input.shape[0])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "        # Compute average epoch loss for training\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (waveform, _, _, metadata) in enumerate(valid_loader):\n",
    "                waveform = waveform.to(device)\n",
    "                stft_spec = stft_transform(waveform)\n",
    "                \n",
    "                # Extract magnitude and phase\n",
    "                magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "                input = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "                labels = torch.stack([one_hot for one_hot in metadata['one_hot_instrument']]).to(device)\n",
    "\n",
    "                output, mu, log_var = model(input, labels)\n",
    "                loss = model.loss_function(input, output, mu, log_var, input.shape[0])\n",
    "                valid_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "        # Compute average validation loss\n",
    "        valid_loss /= len(valid_loader.dataset)        \n",
    "        \n",
    "        # Step the scheduler with validation loss\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        epoch_time = time.time() - start_epoch_time\n",
    "        avg_epoch_time += epoch_time\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, train_loss={train_loss}, valid_loss={valid_loss}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    save_training_results({\n",
    "        \"train_losses\": training_losses,\n",
    "        \"valid_loss\": validation_losses,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"avg_epoch_time\": avg_epoch_time / num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"training_subset_size\": training_subset_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"n_fft\": n_fft,\n",
    "        \"hop_length\": hop_length,\n",
    "        \"input_height\": input_height,\n",
    "        \"input_width\": input_width,\n",
    "        \"latent_dim\": latent_dim,\n",
    "    }, \"cvae.json\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, training_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs, validation_losses, label='Validation Loss', marker='x')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dataset import INSTRUMENT_ID_2_STR, NUM_INSTRUMENTS\n",
    "\n",
    "# Posterior Sampling (conditioned on instrument)\n",
    "model.eval()\n",
    "num_samples = 3  # Number of samples per instrument\n",
    "\n",
    "print(\"Posterior sampling (conditioned on instrument):\")\n",
    "print(\"---------------------------------------------\\n\")\n",
    "\n",
    "# Loop over all instruments\n",
    "for instrument_id in range(NUM_INSTRUMENTS):  # 11 instruments in total\n",
    "    print(f\"\\nGenerating samples for instrument: {INSTRUMENT_ID_2_STR[instrument_id]}\")\n",
    "\n",
    "    # Get a batch from the train_loader (DataLoader is not subscriptable)\n",
    "    batch = next(iter(train_loader))\n",
    "    waveform, _, _, metadata = batch\n",
    "\n",
    "    # Select the first num_samples from the batch\n",
    "    waveform = waveform[:num_samples]\n",
    "    waveform = waveform.to(device)\n",
    "    stft_spec = stft_transform(waveform)\n",
    "\n",
    "    magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "    input_tensor = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "\n",
    "    # Get the one-hot encoded label for the desired instrument (e.g., guitar)\n",
    "    labels = torch.stack([torch.tensor(meta[\"one_hot_instrument\"], dtype=torch.float) for meta in metadata]).to(device)\n",
    "\n",
    "    # Select the one-hot label for the specific instrument_id (e.g., guitar)\n",
    "    selected_label = labels[:num_samples, instrument_id].unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _, _ = model(input_tensor, selected_label)\n",
    "\n",
    "        recon_magnitude = output[:, 0, :, :]    # [batch, freq_bins, time_frames]\n",
    "        recon_phase = output[:, 1, :, :]        # [batch, freq_bins, time_frames]\n",
    "\n",
    "        recon_complex = recon_magnitude * torch.exp(1j * recon_phase)\n",
    "        reconstructed_waveform = istft_transform(recon_complex)  # [batch, time_steps]\n",
    "\n",
    "    # Play the generated samples\n",
    "    for i in range(reconstructed_waveform.shape[0]):\n",
    "        print(f\"\\n=== Generated Sample {i+1} for {INSTRUMENT_ID_2_STR[instrument_id]} ===\")\n",
    "        display(Audio(reconstructed_waveform[i].squeeze().cpu().numpy(), rate=sample_rate))\n",
    "\n",
    "\n",
    "\n",
    "# Prior Sampling (conditioned on instrument)\n",
    "model.eval()\n",
    "num_samples = 3  # Number of samples per instrument\n",
    "\n",
    "print(\"Prior sampling (conditioned on instrument):\")\n",
    "print(\"----------------------------------------\\n\")\n",
    "\n",
    "# Loop over all instruments\n",
    "for instrument_id in range(NUM_INSTRUMENTS):  # 11 instruments in total\n",
    "    print(f\"\\nGenerating samples for instrument: {INSTRUMENT_ID_2_STR[instrument_id]}\")\n",
    "\n",
    "    # Get the one-hot encoded label for the desired instrument (e.g., guitar)\n",
    "    instrument_label = torch.zeros(num_samples, NUM_INSTRUMENTS).to(device)\n",
    "    instrument_label[:, instrument_id] = 1  # Set the label for the specific instrument (e.g., guitar)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Sample from N(0, 1) for the latent space\n",
    "        z = torch.distributions.Normal(0, 1).sample(sample_shape=(num_samples, latent_dim)).to(device)\n",
    "\n",
    "        # Pass the latent vectors and instrument label through the decoder to generate audio\n",
    "        generated_output = model.decoder(z, instrument_label)\n",
    "        generated_output = adjust_shape(generated_output, (input_height, input_width))\n",
    "\n",
    "        # The generated output is in two channels (magnitude and phase), so split them\n",
    "        gen_magnitude = generated_output[:, 0, :, :]\n",
    "        gen_phase = generated_output[:, 1, :, :]\n",
    "\n",
    "        # Recombine into a complex spectrogram\n",
    "        gen_complex = gen_magnitude * torch.exp(1j * gen_phase)\n",
    "        generated_waveforms = istft_transform(gen_complex)  # [num_samples, time_steps]\n",
    "\n",
    "    # Play the generated samples\n",
    "    for i in range(generated_waveforms.shape[0]):\n",
    "        print(f\"\\n=== Generated Sample {i+1} for {INSTRUMENT_ID_2_STR[instrument_id]} ===\")\n",
    "        display(Audio(generated_waveforms[i].squeeze().cpu().numpy(), rate=sample_rate))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
