{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:04:48.243089Z",
     "iopub.status.busy": "2025-03-11T17:04:48.242706Z",
     "iopub.status.idle": "2025-03-11T17:04:48.247995Z",
     "shell.execute_reply": "2025-03-11T17:04:48.247218Z",
     "shell.execute_reply.started": "2025-03-11T17:04:48.243059Z"
    }
   },
   "outputs": [],
   "source": [
    "%run init_notebook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:05:51.213762Z",
     "iopub.status.busy": "2025-03-11T17:05:51.211471Z",
     "iopub.status.idle": "2025-03-11T17:05:51.588454Z",
     "shell.execute_reply": "2025-03-11T17:05:51.587867Z",
     "shell.execute_reply.started": "2025-03-11T17:05:51.213720Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run init_notebook.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchmetrics\n",
    "\n",
    "from src.models import AutoEncoder\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "from src.utils.models import compute_conv2D_output_size, compute_flattened_size\n",
    "\n",
    "# Define parameters\n",
    "input_height = 64\n",
    "input_width = 128\n",
    "latent_dim = 30\n",
    "in_channels = 1\n",
    "filters = [32, 64, 128]\n",
    "\n",
    "model = AutoEncoder(input_height, input_width, latent_dim, in_channels, filters)\n",
    "print(\"AutoEncoder model:\")\n",
    "print(model)\n",
    "\n",
    "# Dummy input tensor: shape [batch_size, in_channels, input_height, input_width]\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "# Dummy input through autoencoder\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "if dummy_input.shape == output.shape:\n",
    "    print(\"Success: Output shape matches input shape.\")\n",
    "else:\n",
    "    print(\"Mismatch: Adjust output_padding in your decoder layers if necessary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:  Encoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=81920, out_features=256, bias=True)\n",
      "  )\n",
      ")\n",
      "Decoder:  Decoder(\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=81920, bias=True)\n",
      "    (1): Unflatten(dim=1, unflattened_size=(256, 10, 32))\n",
      "    (2): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 0))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 0))\n",
      "    (7): ReLU()\n",
      "    (8): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 1/50 [03:41<3:01:06, 221.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss=0.00010799666839691236, valid_loss=7.295537672721386e-05, Time: 221.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 2/50 [07:23<2:57:23, 221.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss=6.324651683771073e-05, valid_loss=6.066344025188763e-05, Time: 221.72s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%run init_notebook.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from IPython.display import Audio, display\n",
    "from torchmetrics import NormalizedRootMeanSquaredError\n",
    "\n",
    "from src.dataset import NSynth   \n",
    "from src.models import AutoEncoder\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "from src.utils.dataset import load_raw_waveform\n",
    "from src.utils.logger import save_training_results\n",
    "\n",
    "# Mel spectrogram with log amplitude (dB)\n",
    "sample_rate = 16000\n",
    "n_fft = 1024\n",
    "hop_length = n_fft // 4\n",
    "n_mels = 80\n",
    "\n",
    "mel_transform = nn.Sequential(\n",
    "    T.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels),\n",
    "    T.AmplitudeToDB(stype=\"power\"),\n",
    ")\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = NSynth(partition='training', transform=mel_transform)\n",
    "valid_dataset = NSynth(partition='validation', transform=mel_transform)\n",
    "test_dataset  = NSynth(partition='testing', transform=mel_transform)\n",
    "\n",
    "# Subset for quicker training\n",
    "batch_size = 64\n",
    "training_subset_size = len(train_dataset) # 50000\n",
    "train_dataset = Subset(train_dataset, list(range(training_subset_size)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader =  DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Model, Optimizer, and Loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_height = n_mels\n",
    "input_width  = 251  # MUST be changed if n_fft or hop_length is modified\n",
    "latent_dim   = 256\n",
    "in_channels  = 1\n",
    "filters      = [64, 128, 256]\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = AutoEncoder(input_height, input_width, latent_dim, in_channels, filters).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = NormalizedRootMeanSquaredError(normalization='l2').to(device) # nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50\n",
    "log_interval = 10\n",
    "avg_epoch_time = 0.0\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate based on validation loss\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "print(f\"Starting training on {device}...\")\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    model.train()\n",
    "    start_epoch_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training Loop\n",
    "    for i, (mel_spec, _, _, _) in enumerate(train_loader):\n",
    "        mel_spec = mel_spec.to(device) # shape [batch_size, 1, n_mels, time_frames]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spec)  # shape [batch_size, 1, n_mels, time_frames]\n",
    "        \n",
    "        loss = criterion(output, mel_spec)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * mel_spec.size(0)\n",
    "\n",
    "    # Compute average epoch loss for training\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (mel_spec, _, _, _) in enumerate(valid_loader):\n",
    "            mel_spec = mel_spec.to(device)\n",
    "            output = model(mel_spec)\n",
    "            loss = criterion(output, mel_spec)\n",
    "            valid_loss += loss.item() * mel_spec.size(0)\n",
    "\n",
    "    # Compute average validation loss\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    \n",
    "    # Step the scheduler with validation loss\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    epoch_time = time.time() - start_epoch_time\n",
    "    avg_epoch_time += epoch_time\n",
    "    print(f\"Epoch {epoch+1}, train_loss={train_loss}, valid_loss={valid_loss}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Save training results after the last epoch\n",
    "    if epoch == num_epochs - 1:\n",
    "        save_training_results({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"avg_epoch_time\": avg_epoch_time / num_epochs,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"training_subset_size\": training_subset_size,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"n_fft\": n_fft,\n",
    "            \"hop_length\": hop_length,\n",
    "            \"n_mels\": n_mels,\n",
    "            \"input_height\": input_height,\n",
    "            \"input_width\": input_width,\n",
    "            \"latent_dim\": latent_dim,\n",
    "            \"in_channels\": in_channels,\n",
    "            \"filters\": filters,\n",
    "        })\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --------------------------\n",
    "# Testing, Inversion, and Audio Playback\n",
    "# --------------------------\n",
    "model.eval()\n",
    "\n",
    "# Samples to compare\n",
    "test_indices = [random.choice(range(len(test_dataset))) for _ in range(10)]\n",
    "\n",
    "for idx in test_indices:\n",
    "    print(f\"\\n=== Test sample index: {idx} ===\")\n",
    "    # (mel_spec, sample_rate, key, metadata) from dataset\n",
    "    mel_spec, sample_rate, key, metadata = test_dataset[idx]\n",
    "\n",
    "    # Listen to the Original Audio with no transform applied\n",
    "    raw_waveform, raw_sr = load_raw_waveform(\"testing\", key)\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"Original audio:\")\n",
    "    display(Audio(raw_waveform.numpy(), rate=raw_sr))\n",
    "\n",
    "    # Reconstruct using the model\n",
    "    mel_spec = mel_spec.unsqueeze(0).to(device)  # shape [1, 1, n_mels, time_frames]\n",
    "    with torch.no_grad():\n",
    "        reconstructed_mel = model(mel_spec)  # shape [1, 1, n_mels, time_frames]\n",
    "\n",
    "    # Convert the reconstructed mel to waveform\n",
    "    recon_np = reconstructed_mel.squeeze().cpu().numpy()  # [n_mels, time_frames]\n",
    "    recon_power = librosa.db_to_power(recon_np)  # dB -> power\n",
    "    reconstructed_audio = librosa.feature.inverse.mel_to_audio(\n",
    "        recon_power, sr=sample_rate, n_fft=n_fft, hop_length=hop_length\n",
    "    )\n",
    "\n",
    "    print(\"Reconstructed audio:\")\n",
    "    display(Audio(reconstructed_audio, rate=raw_sr))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
