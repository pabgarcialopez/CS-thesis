{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:04:48.243089Z",
     "iopub.status.busy": "2025-03-11T17:04:48.242706Z",
     "iopub.status.idle": "2025-03-11T17:04:48.247995Z",
     "shell.execute_reply": "2025-03-11T17:04:48.247218Z",
     "shell.execute_reply.started": "2025-03-11T17:04:48.243059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT ROOT is: c:\\Users\\Articuno\\Desktop\\TFG-info\n"
     ]
    }
   ],
   "source": [
    "%run init_notebook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:05:51.213762Z",
     "iopub.status.busy": "2025-03-11T17:05:51.211471Z",
     "iopub.status.idle": "2025-03-11T17:05:51.588454Z",
     "shell.execute_reply": "2025-03-11T17:05:51.587867Z",
     "shell.execute_reply.started": "2025-03-11T17:05:51.213720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder model:\n",
      "AutoEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "      (7): Linear(in_features=16384, out_features=30, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=30, out_features=16384, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(128, 8, 16))\n",
      "      (2): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "      (8): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Input shape: torch.Size([4, 1, 64, 128])\n",
      "Output shape: torch.Size([4, 1, 64, 128])\n",
      "Success: Output shape matches input shape.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "# Adjust these imports according to your project structure:\n",
    "from src.models import AutoEncoder\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "from src.utils.models import compute_output_size, compute_flattened_size\n",
    "\n",
    "# Define parameters\n",
    "input_height = 64\n",
    "input_width = 128\n",
    "latent_dim = 30\n",
    "in_channels = 1\n",
    "filters = [32, 64, 128]\n",
    "\n",
    "# Instantiate the autoencoder model.\n",
    "# Make sure your AutoEncoder class is defined to accept (input_height, input_width, latent_dim, in_channels, filters)\n",
    "model = AutoEncoder(input_height, input_width, latent_dim, in_channels, filters)\n",
    "print(\"AutoEncoder model:\")\n",
    "print(model)\n",
    "\n",
    "# Create a dummy input tensor: shape [batch_size, in_channels, input_height, input_width]\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "# Pass the dummy input through the autoencoder d\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Print input and output shapes\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Check if the output shape matches the input shape\n",
    "if dummy_input.shape == output.shape:\n",
    "    print(\"Success: Output shape matches input shape.\")\n",
    "else:\n",
    "    print(\"Mismatch: Adjust output_padding in your decoder layers if necessary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  40%|████      | 10/25 [08:41<13:12, 52.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Epoch 1, Step 10/25] loss: 96.7195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  80%|████████  | 20/25 [17:17<04:18, 51.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Epoch 1, Step 20/25] loss: 2.7893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 25/25 [21:09<00:00, 50.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Avg Loss: 17.3753, Time: 1269.48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  40%|████      | 10/25 [08:45<14:26, 57.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Epoch 2, Step 10/25] loss: 12.5235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  40%|████      | 10/25 [09:49<14:43, 58.93s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Use tqdm to track epoch progress\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# (If you prefer a silent loop, remove tqdm and just use 'for i, batch in enumerate(train_loader):')\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# e.g., shape [batch, 1, 64, 128]\u001b[39;49;00m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\src\\dataset.py:31\u001b[0m, in \u001b[0;36mNSynth.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     28\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata[key]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Get the audio file ready for torchaudio\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m wav_file \u001b[38;5;241m=\u001b[39m \u001b[43mget_audio_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# waveform.shape = [num_channels, time] = [1, num_samples = 4 * 16000 = 64000]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(wav_file, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\src\\utils\\dataset.py:79\u001b[0m, in \u001b[0;36mget_audio_file\u001b[1;34m(audio_file_name, partition)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03mReturns the ready-to-use file to be used by `torchaudio.load(...)`\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m zip_path \u001b[38;5;241m=\u001b[39m DATA_PATH \u001b[38;5;241m/\u001b[39m (partition \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m z:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m z\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/audio/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m audio_file:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO(audio_file\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\zipfile.py:1299\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1299\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\zipfile.py:1387\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad offset for central directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1386\u001b[0m fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_dir, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 1387\u001b[0m data \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(size_cd)\n\u001b[0;32m   1388\u001b[0m fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(data)\n\u001b[0;32m   1389\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Adjust these imports according to your project structure:\n",
    "from src.dataset import NSynth      # Your NSynth dataset class\n",
    "from src.models import AutoEncoder  # Your autoencoder model\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "\n",
    "# --------------------------\n",
    "# Define the Mel Spectrogram Transform\n",
    "# --------------------------\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_fft=1024,\n",
    "    hop_length=501,  # So that the mel spectrogram has shape [1, 64, 128]\n",
    "    n_mels=64,\n",
    "    normalized=True # Important\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Create Datasets and DataLoaders\n",
    "# --------------------------\n",
    "train_dataset = NSynth(partition='training', transform=mel_transform)\n",
    "test_dataset  = NSynth(partition='testing', transform=mel_transform)\n",
    "\n",
    "# Optionally, use a subset for quicker debugging:\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(train_dataset, list(range(100)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# --------------------------\n",
    "# Model, Optimizer, and Loss\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_height = 64\n",
    "input_width  = 128\n",
    "latent_dim   = 5\n",
    "in_channels  = 1\n",
    "filters      = [8]\n",
    "\n",
    "model = AutoEncoder(input_height, input_width, latent_dim, in_channels, filters).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Reduce learning rate\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "num_epochs = 20\n",
    "log_interval = 10  # print step info every 10 iterations\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    start_epoch_time = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Use tqdm to track epoch progress\n",
    "    # (If you prefer a silent loop, remove tqdm and just use 'for i, batch in enumerate(train_loader):')\n",
    "    for i, (_, mel_spec, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        mel_spec = mel_spec.to(device)  # e.g., shape [batch, 1, 64, 128]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spec)\n",
    "        loss = criterion(output, mel_spec)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * mel_spec.size(0)\n",
    "\n",
    "        # Print step info every log_interval steps\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(f\"  [Epoch {epoch+1}, Step {i+1}/{len(train_loader)}] loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Compute average epoch loss\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    epoch_time = time.time() - start_epoch_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {epoch_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --------------------------\n",
    "# Testing, Inversion, and Audio Playback\n",
    "# --------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, mel_spec, sample_rate in test_loader:\n",
    "        mel_spec = mel_spec.to(device)  # shape [1, 1, 64, 128]\n",
    "        reconstructed = model(mel_spec)\n",
    "        break\n",
    "\n",
    "print(\"Input shape:\", mel_spec.shape)\n",
    "print(\"Reconstructed shape:\", reconstructed.shape)\n",
    "\n",
    "# Convert the reconstructed mel spectrogram to a numpy array (squeeze batch and channel dims)\n",
    "reconstructed_np = reconstructed.squeeze().cpu().numpy()  # Expected shape: [64, 128]\n",
    "\n",
    "# Inverse Mel Transformation:\n",
    "sr = 16000       # Sample rate\n",
    "n_fft = 1024     # FFT window size\n",
    "hop_length = 501 # Must match mel_transform hop_length\n",
    "n_mels = 64      # Number of mel bins\n",
    "\n",
    "# Convert mel spectrogram back to waveform\n",
    "reconstructed_audio = librosa.feature.inverse.mel_to_audio(\n",
    "    reconstructed_np, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "# Save the audio file\n",
    "sf.write(\"reconstructed.wav\", reconstructed_audio, sr)\n",
    "print(\"Reconstructed audio saved to 'reconstructed.wav'.\")\n",
    "\n",
    "# Play the audio in the notebook\n",
    "Audio(reconstructed_audio, rate=sr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
