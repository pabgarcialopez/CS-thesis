{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:04:48.243089Z",
     "iopub.status.busy": "2025-03-11T17:04:48.242706Z",
     "iopub.status.idle": "2025-03-11T17:04:48.247995Z",
     "shell.execute_reply": "2025-03-11T17:04:48.247218Z",
     "shell.execute_reply.started": "2025-03-11T17:04:48.243059Z"
    }
   },
   "outputs": [],
   "source": [
    "%run init_notebook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:05:51.213762Z",
     "iopub.status.busy": "2025-03-11T17:05:51.211471Z",
     "iopub.status.idle": "2025-03-11T17:05:51.588454Z",
     "shell.execute_reply": "2025-03-11T17:05:51.587867Z",
     "shell.execute_reply.started": "2025-03-11T17:05:51.213720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder model:\n",
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=16384, out_features=30, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=16384, bias=True)\n",
      "    (1): Unflatten(dim=1, unflattened_size=(128, 8, 16))\n",
      "    (2): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Input shape: torch.Size([4, 1, 64, 128])\n",
      "Output shape: torch.Size([4, 1, 64, 128])\n",
      "Success: Output shape matches input shape.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "# Adjust these imports according to your project structure:\n",
    "from src.models import AutoEncoder\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "from src.utils.models import compute_output_size, compute_flattened_size\n",
    "\n",
    "# Define parameters\n",
    "input_height = 64\n",
    "input_width = 128\n",
    "latent_dim = 30\n",
    "in_channels = 1\n",
    "filters = [32, 64, 128]\n",
    "\n",
    "# Instantiate the autoencoder model.\n",
    "# Make sure your AutoEncoder class is defined to accept (input_height, input_width, latent_dim, in_channels, filters)\n",
    "model = AutoEncoder(input_height, input_width, latent_dim, in_channels, filters)\n",
    "print(\"AutoEncoder model:\")\n",
    "print(model)\n",
    "\n",
    "# Create a dummy input tensor: shape [batch_size, in_channels, input_height, input_width]\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "# Pass the dummy input through the autoencoder d\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Print input and output shapes\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Check if the output shape matches the input shape\n",
    "if dummy_input.shape == output.shape:\n",
    "    print(\"Success: Output shape matches input shape.\")\n",
    "else:\n",
    "    print(\"Mismatch: Adjust output_padding in your decoder layers if necessary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   5%|▌         | 1/20 [06:39<2:06:31, 399.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25\n",
      "Epoch 1/20, Loss: 17.3807\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|█         | 2/20 [13:10<1:58:24, 394.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25\n",
      "Epoch 2/20, Loss: 17.3724\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  15%|█▌        | 3/20 [19:43<1:51:31, 393.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25\n",
      "Epoch 3/20, Loss: 17.3681\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 4/20 [26:10<1:44:16, 391.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25\n",
      "Epoch 4/20, Loss: 17.3592\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  25%|██▌       | 5/20 [32:40<1:37:43, 390.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25\n",
      "Epoch 5/20, Loss: 17.3526\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███       | 6/20 [39:13<1:31:19, 391.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25\n",
      "Epoch 6/20, Loss: 17.3452\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  35%|███▌      | 7/20 [45:43<1:24:43, 391.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25\n",
      "Epoch 7/20, Loss: 17.3383\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  35%|███▌      | 7/20 [49:27<1:31:51, 423.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     61\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 62\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteration \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Expected shape: [batch, 1, 64, 128]\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/TFG-info/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.pyenv/versions/TFG-info/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/TFG-info/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/TFG-info/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/Desktop/TFG-info/src/dataset.py:31\u001b[0m, in \u001b[0;36mNSynth.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     28\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata[key]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Get the audio file ready for torchaudio\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m wav_file \u001b[38;5;241m=\u001b[39m \u001b[43mget_audio_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# waveform.shape = [num_channels, time] = [1, num_samples = 4 * 16000 = 64000]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(wav_file, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/TFG-info/src/utils/dataset.py:79\u001b[0m, in \u001b[0;36mget_audio_file\u001b[0;34m(audio_file_name, partition)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mReturns the ready-to-use file to be used by `torchaudio.load(...)`\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m zip_path \u001b[38;5;241m=\u001b[39m DATA_PATH \u001b[38;5;241m/\u001b[39m (partition \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m z:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m z\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/audio/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m audio_file:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO(audio_file\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/zipfile/__init__.py:1349\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/zipfile/__init__.py:1474\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1471\u001b[0m x\u001b[38;5;241m.\u001b[39m_raw_time \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m   1472\u001b[0m x\u001b[38;5;241m.\u001b[39mdate_time \u001b[38;5;241m=\u001b[39m ( (d\u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m9\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1980\u001b[39m, (d\u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0xF\u001b[39m, d\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0x1F\u001b[39m,\n\u001b[1;32m   1473\u001b[0m                 t\u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m11\u001b[39m, (t\u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0x3F\u001b[39m, (t\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0x1F\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m )\n\u001b[0;32m-> 1474\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decodeExtra\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_filename_crc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m x\u001b[38;5;241m.\u001b[39mheader_offset \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mheader_offset \u001b[38;5;241m+\u001b[39m concat\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelist\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/zipfile/__init__.py:505\u001b[0m, in \u001b[0;36mZipInfo._decodeExtra\u001b[0;34m(self, filename_crc)\u001b[0m\n\u001b[1;32m    503\u001b[0m unpack \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(extra) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 505\u001b[0m     tp, ln \u001b[38;5;241m=\u001b[39m \u001b[43munpack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<HH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ln\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(extra):\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrupt extra field \u001b[39m\u001b[38;5;132;01m%04x\u001b[39;00m\u001b[38;5;124m (size=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tp, ln))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Adjust these imports according to your project structure:\n",
    "from src.dataset import NSynth      # Your NSynth dataset class\n",
    "from src.models import AutoEncoder  # Your autoencoder model\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "\n",
    "# --------------------------\n",
    "# Define the Mel Spectrogram Transform\n",
    "# --------------------------\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_fft=1024,\n",
    "    hop_length=501,  # So that the mel spectrogram has shape [1, 64, 128]\n",
    "    n_mels=64,\n",
    "    normalized=True # Important\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Create Datasets and DataLoaders\n",
    "# --------------------------\n",
    "train_dataset = NSynth(partition='training', transform=mel_transform)\n",
    "test_dataset  = NSynth(partition='testing', transform=mel_transform)\n",
    "\n",
    "# Optionally, use a subset for quicker debugging:\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(train_dataset, list(range(100)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# --------------------------\n",
    "# Model, Optimizer, and Loss\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_height = 64\n",
    "input_width  = 128\n",
    "latent_dim   = 5\n",
    "in_channels  = 1\n",
    "filters      = [8]\n",
    "\n",
    "model = AutoEncoder(input_height, input_width, latent_dim, in_channels, filters).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Reduce learning rate\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "num_epochs = 20\n",
    "print(\"Starting training...\")\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i, (_, mel_spec, _) in enumerate(train_loader):\n",
    "        print(f\"Iteration {i+1}\")\n",
    "        mel_spec = mel_spec.to(device)  # Expected shape: [batch, 1, 64, 128]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spec)\n",
    "        loss = criterion(output, mel_spec)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * mel_spec.size(0)\n",
    "\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --------------------------\n",
    "# Testing, Inversion, and Audio Playback\n",
    "# --------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, mel_spec, sample_rate in test_loader:\n",
    "        mel_spec = mel_spec.to(device)  # shape [1, 1, 64, 128]\n",
    "        reconstructed = model(mel_spec)\n",
    "        break\n",
    "\n",
    "print(\"Input shape:\", mel_spec.shape)\n",
    "print(\"Reconstructed shape:\", reconstructed.shape)\n",
    "\n",
    "# Convert the reconstructed mel spectrogram to a numpy array (squeeze batch and channel dims)\n",
    "reconstructed_np = reconstructed.squeeze().cpu().numpy()  # Expected shape: [64, 128]\n",
    "\n",
    "# Inverse Mel Transformation:\n",
    "sr = 16000       # Sample rate\n",
    "n_fft = 1024     # FFT window size\n",
    "hop_length = 501 # Must match mel_transform hop_length\n",
    "n_mels = 64      # Number of mel bins\n",
    "\n",
    "# Convert mel spectrogram back to waveform\n",
    "reconstructed_audio = librosa.feature.inverse.mel_to_audio(\n",
    "    reconstructed_np, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "# Save the audio file\n",
    "sf.write(\"reconstructed.wav\", reconstructed_audio, sr)\n",
    "print(\"Reconstructed audio saved to 'reconstructed.wav'.\")\n",
    "\n",
    "# Play the audio in the notebook\n",
    "Audio(reconstructed_audio, rate=sr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
