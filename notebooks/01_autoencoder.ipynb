{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:05:51.213762Z",
     "iopub.status.busy": "2025-03-11T17:05:51.211471Z",
     "iopub.status.idle": "2025-03-11T17:05:51.588454Z",
     "shell.execute_reply": "2025-03-11T17:05:51.587867Z",
     "shell.execute_reply.started": "2025-03-11T17:05:51.213720Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run init_notebook.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchmetrics\n",
    "\n",
    "from src.models import AutoEncoder\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "from src.utils.models import compute_conv2D_output_size, compute_flattened_size\n",
    "\n",
    "# Define parameters\n",
    "input_height = 64\n",
    "input_width = 128\n",
    "latent_dim = 30\n",
    "in_channels = 1\n",
    "filters = [32, 64, 128]\n",
    "\n",
    "model = AutoEncoder(input_height, input_width, latent_dim, in_channels, filters)\n",
    "print(\"AutoEncoder model:\")\n",
    "print(model)\n",
    "\n",
    "# Dummy input tensor: shape [batch_size, in_channels, input_height, input_width]\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "# Dummy input through autoencoder\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "if dummy_input.shape == output.shape:\n",
    "    print(\"Success: Output shape matches input shape.\")\n",
    "else:\n",
    "    print(\"Mismatch: Adjust output_padding in your decoder layers if necessary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:  Encoder(\n",
      "  (encoder_blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=516096, out_features=128, bias=True)\n",
      ")\n",
      "Decoder:  Decoder(\n",
      "  (fc2): Linear(in_features=128, out_features=516096, bias=True)\n",
      "  (unflatten): Unflatten(dim=1, unflattened_size=(128, 64, 63))\n",
      "  (deconv_blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(32, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|‚ñè         | 1/50 [07:11<5:52:17, 431.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss=2.4877455362019965, valid_loss=2.2662928536555467, Time: 431.37s\n"
     ]
    }
   ],
   "source": [
    "%run init_notebook.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import time\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from IPython.display import Audio, display\n",
    "from torchmetrics import NormalizedRootMeanSquaredError\n",
    "\n",
    "from src.dataset import NSynth   \n",
    "from src.models import AutoEncoder\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "from src.utils.dataset import load_raw_waveform\n",
    "from src.utils.logger import save_training_results\n",
    "from src.utils.models import compute_magnitude_and_phase\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# STFT transform\n",
    "sample_rate = 16000\n",
    "n_fft = 512\n",
    "hop_length = 128\n",
    "win_length = n_fft  # Same as n_fft\n",
    "\n",
    "# Apply the correct transform for magnitude and phase (onesided=False to handle complex spectrogram)\n",
    "stft_transform = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    power=None,  # Keep as complex spectrogram (magnitude and phase)\n",
    "    onesided=False,  # Make sure we keep the full spectrum (complex-valued)\n",
    "    center=False\n",
    ").to(device)\n",
    "\n",
    "# Define the inverse STFT function (onesided=False)\n",
    "istft_transform = torchaudio.transforms.InverseSpectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    onesided=False  # Make sure we reconstruct using the full complex spectrogram\n",
    ").to(device)\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = NSynth(partition='training')\n",
    "valid_dataset = NSynth(partition='validation')\n",
    "test_dataset  = NSynth(partition='testing')\n",
    "\n",
    "# Subset for quicker training\n",
    "batch_size = 64\n",
    "training_subset_size = len(train_dataset)\n",
    "train_dataset = Subset(train_dataset, list(range(training_subset_size)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Model, Optimizer, and Loss\n",
    "\n",
    "input_height = 512 \n",
    "input_width  = 497\n",
    "latent_dim   = 128\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = AutoEncoder((input_height, input_width), latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss().to(device) # NormalizedRootMeanSquaredError(normalization='l2').to(device)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50\n",
    "log_interval = 10\n",
    "avg_epoch_time = 0.0\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate based on validation loss\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "print(f\"Starting training on {device}...\")\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    model.train()\n",
    "    start_epoch_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training Loop\n",
    "    for i, (waveform, _, _, _) in enumerate(train_loader):\n",
    "        # waveform has shape [batch_size, 1, time_steps]\n",
    "\n",
    "        waveform = waveform.to(device)\n",
    "\n",
    "        # Apply STFT transformation\n",
    "        stft_spec = stft_transform(waveform)  # [batch_size, 1, freq_bins, time_frames]\n",
    "\n",
    "        # Extract magnitude and phase\n",
    "        magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "\n",
    "        input_data = torch.cat([magnitude, phase], dim=1).to(device)  # [batch_size, 2, freq_bins, time_frames]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data)  # shape [batch_size, 2, freq_bins, time_frames]\n",
    "\n",
    "        loss = criterion(output, input_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "    # Compute average epoch loss for training\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (waveform, _, _, _) in enumerate(valid_loader):\n",
    "            waveform = waveform.to(device)\n",
    "            stft_spec = stft_transform(waveform)\n",
    "            \n",
    "            # Extract magnitude and phase\n",
    "            magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "            input_data = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "\n",
    "            output = model(input_data)\n",
    "            loss = criterion(output, input_data)\n",
    "            valid_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "    # Compute average validation loss\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    \n",
    "    # Step the scheduler with validation loss\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    epoch_time = time.time() - start_epoch_time\n",
    "    avg_epoch_time += epoch_time\n",
    "    print(f\"Epoch {epoch+1}, train_loss={train_loss}, valid_loss={valid_loss}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Save training results after the last epoch\n",
    "    if epoch == num_epochs - 1:\n",
    "        save_training_results({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"avg_epoch_time\": avg_epoch_time / num_epochs,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"training_subset_size\": training_subset_size,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"n_fft\": n_fft,\n",
    "            \"hop_length\": hop_length,\n",
    "            \"input_height\": input_height,\n",
    "            \"input_width\": input_width,\n",
    "            \"latent_dim\": latent_dim,\n",
    "        })\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Testing, Inversion, and Audio Playback\n",
    "# --------------------------\n",
    "model.eval()\n",
    "\n",
    "# Samples to compare\n",
    "test_indices = [random.choice(range(len(test_dataset))) for _ in range(10)]\n",
    "\n",
    "for idx in test_indices:\n",
    "    print(f\"\\n=== Test sample index: {idx} ===\")\n",
    "    # (stft_spec, sample_rate, key, metadata) from dataset; here we only need key and sample_rate\n",
    "    _, sample_rate, key, metadata = test_dataset[idx]\n",
    "\n",
    "    # Listen to the Original Audio with no transform applied\n",
    "    raw_waveform, raw_sr = load_raw_waveform(\"testing\", key)\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"Original audio:\")\n",
    "    display(Audio(raw_waveform.numpy(), rate=raw_sr))\n",
    "\n",
    "    # Ensure raw_waveform has a batch and channel dimension, expected shape: [batch, 1, time_steps]\n",
    "    if raw_waveform.ndim == 2:  # e.g., [1, time_steps]\n",
    "        raw_waveform = raw_waveform.unsqueeze(0)\n",
    "    \n",
    "    raw_waveform = raw_waveform.to(device)\n",
    "\n",
    "    # Apply STFT transformation\n",
    "    stft_spec = stft_transform(raw_waveform)  # [batch, 1, freq_bins, time_frames]\n",
    "    \n",
    "    # Extract magnitude and phase using the helper function\n",
    "    magnitude, phase = compute_magnitude_and_phase(stft_spec)  # both: [batch, 1, freq_bins, time_frames]\n",
    "    input_data = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "    \n",
    "    # Reconstruct using the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)  # shape: [batch, 2, freq_bins, time_frames]\n",
    "    \n",
    "    # Split the network output into magnitude and phase\n",
    "    recon_magnitude = output[:, 0, :, :]  # [batch, freq_bins, time_frames]\n",
    "    recon_phase = output[:, 1, :, :]          # [batch, freq_bins, time_frames]\n",
    "\n",
    "    if torch.isnan(recon_magnitude).any() or torch.isinf(recon_magnitude).any():\n",
    "        print(\"NaN or Inf detected in magnitude output!\")\n",
    "    if torch.isnan(recon_phase).any() or torch.isinf(recon_phase).any():\n",
    "        print(\"NaN or Inf detected in phase output!\")\n",
    "    # Recombine into a complex spectrogram\n",
    "    recon_complex = recon_magnitude * torch.exp(1j * recon_phase)\n",
    "    \n",
    "    # Apply the inverse STFT transformation to get the waveform\n",
    "    reconstructed_waveform = istft_transform(recon_complex)  # [batch, time_steps]\n",
    "    \n",
    "    print(\"Reconstructed audio:\")\n",
    "    display(Audio(reconstructed_waveform.squeeze().cpu().numpy(), rate=raw_sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
