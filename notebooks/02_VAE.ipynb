{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:  Encoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(2, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=258048, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=258048, out_features=128, bias=True)\n",
      ")\n",
      "Decoder:  Decoder(\n",
      "  (fc2): Linear(in_features=128, out_features=258048, bias=True)\n",
      "  (unflatten): Unflatten(dim=1, unflattened_size=(64, 64, 63))\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "No saved model found, starting training from scratch.\n",
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 1/50 [04:41<3:50:17, 281.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss=2.529498975570967, valid_loss=1.979893374243714, Time: 281.98s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 2/50 [09:23<3:45:10, 281.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss=1.9103410515209214, valid_loss=1.7998058429970165, Time: 281.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 3/50 [14:02<3:39:46, 280.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train_loss=1.7818085164640132, valid_loss=1.7181159856097896, Time: 279.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 4/50 [18:36<3:33:01, 277.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train_loss=1.7140659897025818, valid_loss=1.6748974957776797, Time: 273.69s\n"
     ]
    }
   ],
   "source": [
    "%run init_notebook.py\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import time\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from src.dataset import NSynth   \n",
    "from src.models import VAE\n",
    "from src.utils.models import adjust_shape\n",
    "from src.utils.logger import save_training_results\n",
    "from src.utils.models import compute_magnitude_and_phase\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = r\"C:\\Users\\Articuno\\Desktop\\TFG-info\\data\\models\\vae.pth\"\n",
    "\n",
    "# STFT transform\n",
    "sample_rate = 16000\n",
    "n_fft = 512\n",
    "hop_length = 128\n",
    "win_length = n_fft  # Same as n_fft\n",
    "\n",
    "# Apply the correct transform for magnitude and phase (onesided=False to handle complex spectrogram)\n",
    "stft_transform = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    power=None,  # Keep as complex spectrogram (magnitude and phase)\n",
    "    onesided=False,  # Make sure we keep the full spectrum (complex-valued)\n",
    "    center=False\n",
    ").to(device)\n",
    "\n",
    "# Define the inverse STFT function (onesided=False)\n",
    "istft_transform = torchaudio.transforms.InverseSpectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    onesided=False  # Make sure we reconstruct using the full complex spectrogram\n",
    ").to(device)\n",
    "\n",
    "# Datasets and DataLoaders, training parameters.\n",
    "train_dataset = NSynth(partition='training')\n",
    "valid_dataset = NSynth(partition='validation')\n",
    "test_dataset  = NSynth(partition='testing')\n",
    "\n",
    "batch_size = 64\n",
    "training_subset_size = len(train_dataset)\n",
    "train_dataset = Subset(train_dataset, list(range(training_subset_size)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "input_height = 512 \n",
    "input_width  = 497\n",
    "latent_dim   = 128\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = VAE((input_height, input_width), latent_dim).to(device)\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}. Unable to load the model.\")\n",
    "else:\n",
    "    print(\"No saved model found, starting training from scratch.\")\n",
    "\n",
    "    # Model, Optimizer, and Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 50\n",
    "    log_interval = 10\n",
    "    avg_epoch_time = 0.0\n",
    "\n",
    "    # Learning rate scheduler to reduce learning rate based on validation loss\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    print(f\"Starting training on {device}...\")\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        model.train()\n",
    "        start_epoch_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Training Loop\n",
    "        for i, (waveform, _, _, _) in enumerate(train_loader):\n",
    "            # waveform has shape [batch_size, 1, time_steps]\n",
    "\n",
    "            waveform = waveform.to(device)\n",
    "\n",
    "            # Apply STFT transformation\n",
    "            stft_spec = stft_transform(waveform)  # [batch_size, 1, freq_bins, time_frames]\n",
    "\n",
    "            # Extract magnitude and phase\n",
    "            magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "\n",
    "            input = torch.cat([magnitude, phase], dim=1).to(device)  # [batch_size, 2, freq_bins, time_frames]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, mu, log_var = model(input)  # shape [batch_size, 2, freq_bins, time_frames]\n",
    "\n",
    "            loss = model.loss_function(input, output, mu, log_var, input.shape[0])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "        # Compute average epoch loss for training\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (waveform, _, _, _) in enumerate(valid_loader):\n",
    "                waveform = waveform.to(device)\n",
    "                stft_spec = stft_transform(waveform)\n",
    "                \n",
    "                # Extract magnitude and phase\n",
    "                magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "                input = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "\n",
    "                output, mu, log_var = model(input)\n",
    "                loss = model.loss_function(input, output, mu, log_var, input.shape[0])\n",
    "                valid_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "        # Compute average validation loss\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        \n",
    "        # Step the scheduler with validation loss\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        epoch_time = time.time() - start_epoch_time\n",
    "        avg_epoch_time += epoch_time\n",
    "        print(f\"Epoch {epoch+1}, train_loss={train_loss}, valid_loss={valid_loss}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # Save training results\n",
    "        if epoch == num_epochs - 1:\n",
    "            save_training_results({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": valid_loss,\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"avg_epoch_time\": avg_epoch_time / num_epochs,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"training_subset_size\": training_subset_size,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"sample_rate\": sample_rate,\n",
    "                \"n_fft\": n_fft,\n",
    "                \"hop_length\": hop_length,\n",
    "                \"input_height\": input_height,\n",
    "                \"input_width\": input_width,\n",
    "                \"latent_dim\": latent_dim,\n",
    "            })\n",
    "\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# ------------\n",
    "# Sampling\n",
    "# ------------\n",
    "model.eval()\n",
    "num_samples = 10  # Number of samples to generate\n",
    "\n",
    "# POSTERIOR SAMPLING\n",
    "print(\"Posterior sampling:\")\n",
    "print(\"-------------------\\n\")\n",
    "\n",
    "# Get a batch from the train_loader (DataLoader is not subscriptable)\n",
    "batch = next(iter(train_loader))\n",
    "waveform, _, _, _ = batch\n",
    "\n",
    "# Select the first num_samples from the batch\n",
    "waveform = waveform[:num_samples]\n",
    "waveform = waveform.to(device)\n",
    "stft_spec = stft_transform(waveform)\n",
    "\n",
    "magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "input_tensor = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, _, _ = model(input_tensor)\n",
    "\n",
    "    recon_magnitude = output[:, 0, :, :]    # [batch, freq_bins, time_frames]\n",
    "    recon_phase = output[:, 1, :, :]        # [batch, freq_bins, time_frames]\n",
    "\n",
    "    recon_complex = recon_magnitude * torch.exp(1j * recon_phase)\n",
    "    reconstructed_waveform = istft_transform(recon_complex)  # [batch, time_steps]\n",
    "\n",
    "for i in range(reconstructed_waveform.shape[0]):\n",
    "    print(f\"\\n=== Generated Sample {i+1} ===\")\n",
    "    display(Audio(reconstructed_waveform[i].squeeze().cpu().numpy(), rate=sample_rate))\n",
    "\n",
    "\n",
    "# PRIOR SAMPLING\n",
    "print(\"Prior sampling:\")\n",
    "print(\"---------------\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample from N(0, 1)\n",
    "    z = torch.distributions.Normal(0, 1).sample(sample_shape=(num_samples, latent_dim)).to(device)\n",
    "    \n",
    "    # Pass the latent vectors through the decoder to generate audio\n",
    "    with torch.no_grad():\n",
    "        generated_output = model.decoder(z)\n",
    "        generated_output = adjust_shape(generated_output, (input_height, input_width))\n",
    "\n",
    "    # The generated output is in two channels (magnitude and phase), so split them\n",
    "    gen_magnitude = generated_output[:, 0, :, :]\n",
    "    gen_phase = generated_output[:, 1, :, :]\n",
    "    \n",
    "    # Recombine into a complex spectrogram\n",
    "    gen_complex = gen_magnitude * torch.exp(1j * gen_phase)\n",
    "    generated_waveforms = istft_transform(gen_complex) # [num_samples, time_steps]\n",
    "\n",
    "# Play the generated samples\n",
    "for i in range(generated_waveforms.shape[0]):\n",
    "    print(f\"\\n=== Generated Sample {i+1} ===\")\n",
    "    display(Audio(generated_waveforms[i].squeeze().cpu().numpy(), rate=sample_rate))         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
