{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:  Encoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(2, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=512000, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=512000, out_features=256, bias=True)\n",
      ")\n",
      "Decoder:  Decoder(\n",
      "  (fc2): Linear(in_features=256, out_features=512000, bias=True)\n",
      "  (unflatten): Unflatten(dim=1, unflattened_size=(64, 100, 80))\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "No saved model found, starting training from scratch.\n",
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%run init_notebook.py\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import time\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from src.dataset import NSynth   \n",
    "from src.models import VAE\n",
    "from src.utils.models import adjust_shape\n",
    "from src.utils.logger import save_training_results\n",
    "from src.utils.models import compute_magnitude_and_phase\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = r\"C:\\Users\\Articuno\\Desktop\\TFG-info\\data\\models\\vae.pth\"\n",
    "\n",
    "# STFT transform\n",
    "sample_rate = 16000\n",
    "n_fft = 800\n",
    "hop_length = 100\n",
    "win_length = n_fft  # Same as n_fft\n",
    "\n",
    "# Apply the correct transform for magnitude and phase (onesided=False to handle complex spectrogram)\n",
    "stft_transform = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    power=None,  # Keep as complex spectrogram (magnitude and phase)\n",
    "    onesided=False,  # Make sure we keep the full spectrum (complex-valued)\n",
    "    center=False\n",
    ").to(device)\n",
    "\n",
    "# Define the inverse STFT function (onesided=False)\n",
    "istft_transform = torchaudio.transforms.InverseSpectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    onesided=False  # Make sure we reconstruct using the full complex spectrogram\n",
    ").to(device)\n",
    "\n",
    "# Datasets and DataLoaders, training parameters.\n",
    "train_dataset = NSynth(partition='training')\n",
    "valid_dataset = NSynth(partition='validation')\n",
    "test_dataset  = NSynth(partition='testing')\n",
    "\n",
    "batch_size = 64\n",
    "training_subset_size = len(train_dataset)\n",
    "train_dataset = Subset(train_dataset, list(range(training_subset_size)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "input_height = 800 \n",
    "input_width  = 633\n",
    "latent_dim   = 256\n",
    "learning_rate = 1.5e-3\n",
    "\n",
    "model = VAE((input_height, input_width), latent_dim).to(device)\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}. Unable to load the model.\")\n",
    "else:\n",
    "    print(\"No saved model found, starting training from scratch.\")\n",
    "\n",
    "    # Model, Optimizer, and Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 30\n",
    "    log_interval = 10\n",
    "    avg_epoch_time = 0.0\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Learning rate scheduler to reduce learning rate based on validation loss\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    print(f\"Starting training on {device}...\")\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "\n",
    "        model.train()\n",
    "        start_epoch_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Training Loop\n",
    "        for i, (waveform, _, _, _) in enumerate(train_loader):\n",
    "            # waveform has shape [batch_size, 1, time_steps]\n",
    "\n",
    "            waveform = waveform.to(device)\n",
    "\n",
    "            # Apply STFT transformation\n",
    "            stft_spec = stft_transform(waveform)  # [batch_size, 1, freq_bins, time_frames]\n",
    "\n",
    "            # Extract magnitude and phase\n",
    "            magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "\n",
    "            input = torch.cat([magnitude, phase], dim=1).to(device)  # [batch_size, 2, freq_bins, time_frames]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, mu, log_var = model(input)  # shape [batch_size, 2, freq_bins, time_frames]\n",
    "\n",
    "            loss = model.loss_function(input, output, mu, log_var, input.shape[0])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "        # Compute average epoch loss for training\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (waveform, _, _, _) in enumerate(valid_loader):\n",
    "                waveform = waveform.to(device)\n",
    "                stft_spec = stft_transform(waveform)\n",
    "                \n",
    "                # Extract magnitude and phase\n",
    "                magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "                input = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "\n",
    "                output, mu, log_var = model(input)\n",
    "                loss = model.loss_function(input, output, mu, log_var, input.shape[0])\n",
    "                valid_loss += loss.item() * waveform.size(0)\n",
    "\n",
    "        # Compute average validation loss\n",
    "        valid_loss /= len(valid_loader.dataset)        \n",
    "        \n",
    "        # Step the scheduler with validation loss\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        epoch_time = time.time() - start_epoch_time\n",
    "        avg_epoch_time += epoch_time\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, train_loss={train_loss}, valid_loss={valid_loss}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    save_training_results({\n",
    "        \"train_losses\": training_losses,\n",
    "        \"valid_loss\": validation_losses,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"avg_epoch_time\": avg_epoch_time / num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"training_subset_size\": training_subset_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"n_fft\": n_fft,\n",
    "        \"hop_length\": hop_length,\n",
    "        \"input_height\": input_height,\n",
    "        \"input_width\": input_width,\n",
    "        \"latent_dim\": latent_dim,\n",
    "    }, \"vae.json\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, training_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs, validation_losses, label='Validation Loss', marker='x')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Accuracy\n",
    "\n",
    "The **reconstruction accuracy** for each sample is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy}_i = \\left(1 - \\frac{\\lVert x_i - \\hat{x}_i \\rVert_2^2}{\\lVert x_i \\rVert_2^2 + \\varepsilon} \\right) \\times 100\\%\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i$ is the input for sample $i$ (of shape `[batch_size, 2, num_freq, time_frames]`),\n",
    "- $\\hat{x}_i$ is the reconstructed output from the model,\n",
    "- $\\lVert \\cdot \\rVert_2^2$ denotes the squared L2 norm (i.e., the sum of squared elements),\n",
    "- $\\varepsilon$ is a small constant to avoid division by zero.\n",
    "\n",
    "This gives a value between 0% and 100% for each sample.\n",
    "\n",
    "---\n",
    "\n",
    "### Total Accuracy\n",
    "\n",
    "To compute the **overall accuracy** across all samples in the dataset (or batch-wise using a DataLoader), we take the average of all per-sample accuracies:\n",
    "\n",
    "$$\n",
    "\\text{Total Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Accuracy}_i\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of samples evaluated.\n",
    "\n",
    "This metric tells us how well the model reconstructs the inputs **on average** over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 49.16%\n"
     ]
    }
   ],
   "source": [
    "def reconstruction_accuracy(x, x_hat):\n",
    "    # Compute per-sample accuracy tensor of shape [B]\n",
    "    numerator = torch.sum((x - x_hat) ** 2, dim=(1, 2, 3))\n",
    "    denominator = torch.sum(x ** 2, dim=(1, 2, 3)) + 1e-8\n",
    "    accuracy = 1 - numerator / denominator\n",
    "    return torch.clamp(accuracy, min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "total_acc_sum = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (waveform, _, _, _) in test_loader:\n",
    "        raw_waveform = waveform.to(device)  # shape: [B, 1, T]\n",
    "\n",
    "        stft_spec = stft_transform(raw_waveform)\n",
    "        magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "        input = torch.cat([magnitude, phase], dim=1)\n",
    "\n",
    "        output, _, _ = model(input)\n",
    "\n",
    "        batch_acc = reconstruction_accuracy(input, output) \n",
    "        total_acc_sum += batch_acc.sum().item()\n",
    "        total_samples += waveform.size(0)\n",
    "\n",
    "test_accuracy = round((total_acc_sum / total_samples) * 100, 2)\n",
    "print(f\"Testing accuracy: {test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the VAE\n",
    "\n",
    "This section demonstrates two types of sampling using the trained Variational Autoencoder (VAE):\n",
    "\n",
    "---\n",
    "\n",
    "### Posterior Sampling\n",
    "\n",
    "We sample from the **posterior distribution** by encoding real data samples through the encoder, and then decoding the resulting latent representations.\n",
    "\n",
    "Steps:\n",
    "1. Take a batch of real waveforms from the training set.\n",
    "2. Compute their STFT representations (magnitude and phase).\n",
    "3. Concatenate both as input to the VAE.\n",
    "4. The VAE reconstructs magnitude and phase via the decoder.\n",
    "5. Convert reconstructed spectrograms back to waveforms using inverse STFT.\n",
    "\n",
    "This gives us reconstructions of real samples, useful for qualitative inspection of the VAE's reconstruction ability.\n",
    "\n",
    "---\n",
    "\n",
    "### Prior Sampling\n",
    "\n",
    "We sample directly from the **prior distribution** over the latent space, which is assumed to be standard Gaussian $ \\mathcal{N}(0, I) $.\n",
    "\n",
    "Steps:\n",
    "1. Sample random latent vectors $ z \\sim \\mathcal{N}(0, I) $.\n",
    "2. Pass these vectors through the decoder.\n",
    "3. The output is a pair of magnitude and phase spectrograms.\n",
    "4. Recombine into complex spectrograms and apply inverse STFT to get raw audio waveforms.\n",
    "\n",
    "This tests how well the decoder has learned to generate plausible samples from the latent space.\n",
    "\n",
    "---\n",
    "\n",
    "Both types of samples are played back using IPythonâ€™s `Audio` widget for auditory inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Sampling\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of samples to generate\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# POSTERIOR SAMPLING\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# Sampling\n",
    "# ------------\n",
    "model.eval()\n",
    "num_samples = 10  # Number of samples to generate\n",
    "\n",
    "# POSTERIOR SAMPLING\n",
    "print(\"Posterior sampling:\")\n",
    "print(\"-------------------\\n\")\n",
    "\n",
    "# Get a batch from the train_loader (DataLoader is not subscriptable)\n",
    "batch = next(iter(train_loader))\n",
    "waveform, _, _, _ = batch\n",
    "\n",
    "# Select the first num_samples from the batch\n",
    "waveform = waveform[:num_samples]\n",
    "waveform = waveform.to(device)\n",
    "stft_spec = stft_transform(waveform)\n",
    "\n",
    "magnitude, phase = compute_magnitude_and_phase(stft_spec)\n",
    "input_tensor = torch.cat([magnitude, phase], dim=1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, _, _ = model(input_tensor)\n",
    "\n",
    "    recon_magnitude = output[:, 0, :, :]    # [batch, freq_bins, time_frames]\n",
    "    recon_phase = output[:, 1, :, :]        # [batch, freq_bins, time_frames]\n",
    "\n",
    "    recon_complex = recon_magnitude * torch.exp(1j * recon_phase)\n",
    "    reconstructed_waveform = istft_transform(recon_complex)  # [batch, time_steps]\n",
    "\n",
    "for i in range(reconstructed_waveform.shape[0]):\n",
    "    print(f\"\\n=== Generated Sample {i+1} ===\")\n",
    "    display(Audio(reconstructed_waveform[i].squeeze().cpu().numpy(), rate=sample_rate))\n",
    "\n",
    "\n",
    "# PRIOR SAMPLING\n",
    "print(\"Prior sampling:\")\n",
    "print(\"---------------\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample from N(0, 1)\n",
    "    z = torch.distributions.Normal(0, 1).sample(sample_shape=(num_samples, latent_dim)).to(device)\n",
    "    \n",
    "    # Pass the latent vectors through the decoder to generate audio\n",
    "    with torch.no_grad():\n",
    "        generated_output = model.decoder(z)\n",
    "        generated_output = adjust_shape(generated_output, (input_height, input_width))\n",
    "\n",
    "    # The generated output is in two channels (magnitude and phase), so split them\n",
    "    gen_magnitude = generated_output[:, 0, :, :]\n",
    "    gen_phase = generated_output[:, 1, :, :]\n",
    "    \n",
    "    # Recombine into a complex spectrogram\n",
    "    gen_complex = gen_magnitude * torch.exp(1j * gen_phase)\n",
    "    generated_waveforms = istft_transform(gen_complex) # [num_samples, time_steps]\n",
    "\n",
    "# Play the generated samples\n",
    "for i in range(generated_waveforms.shape[0]):\n",
    "    print(f\"\\n=== Generated Sample {i+1} ===\")\n",
    "    display(Audio(generated_waveforms[i].squeeze().cpu().numpy(), rate=sample_rate))         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
