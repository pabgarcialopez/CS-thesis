{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(128, 251), (64, 126)]\n",
      "[(128, 251), (64, 126), (32, 63)]\n",
      "[(128, 251), (64, 126), (32, 63), (16, 32)]\n",
      "Encoder:  Encoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=65536, out_features=512, bias=True)\n",
      "  )\n",
      ")\n",
      "Decoder:  Decoder(\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=65536, bias=True)\n",
      "    (1): Unflatten(dim=1, unflattened_size=(128, 16, 32))\n",
      "    (2): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 0))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 0))\n",
      "    (7): ReLU()\n",
      "    (8): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m target_waveform \u001b[38;5;241m=\u001b[39m inverse_mel(mel_spec)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Calculate the loss between the reconstructed waveform and the target mel spectrogram\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_waveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_waveform\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use MultiResolutionSTFTLoss here\u001b[39;00m\n\u001b[0;32m    100\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Articuno\\Desktop\\TFG-info\\TFG-info-venv\\Lib\\site-packages\\auraloss\\freq.py:210\u001b[0m, in \u001b[0;36mSTFTLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor, target: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 210\u001b[0m     bs, chs, seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperceptual_weighting:  \u001b[38;5;66;03m# apply optional A-weighting via FIR filter\u001b[39;00m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;66;03m# since FIRFilter only support mono audio we will move channels to batch dim\u001b[39;00m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(bs \u001b[38;5;241m*\u001b[39m chs, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from IPython.display import Audio, display\n",
    "from torchmetrics import NormalizedRootMeanSquaredError\n",
    "import auraloss  # Import auraloss for MultiResolutionSTFTLoss\n",
    "\n",
    "from src.dataset import NSynth   \n",
    "from src.models import AutoEncoder\n",
    "from src.config import CONV_KERNEL_SIZE, CONV_STRIDE, CONV_PADDING\n",
    "from src.utils.dataset import load_raw_waveform\n",
    "from src.utils.logger import save_training_results\n",
    "\n",
    "# Mel spectrogram with log amplitude (dB) and Z-score normalization\n",
    "sample_rate = 16000\n",
    "n_fft = 1024\n",
    "hop_length = n_fft // 4\n",
    "n_mels = 128\n",
    "\n",
    "mel_transform = nn.Sequential(\n",
    "    T.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels),\n",
    "    T.AmplitudeToDB(stype=\"power\"),\n",
    ")\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = NSynth(partition='training', transform=mel_transform)\n",
    "valid_dataset = NSynth(partition='validation', transform=mel_transform)\n",
    "test_dataset  = NSynth(partition='testing', transform=mel_transform)\n",
    "\n",
    "# Subset for quicker training\n",
    "batch_size = 64\n",
    "training_subset_size = 70000 # len(train_dataset) # 50000\n",
    "train_dataset = Subset(train_dataset, list(range(training_subset_size)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader =  DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Model, Optimizer, and Loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_height = n_mels\n",
    "input_width  = 251\n",
    "latent_dim   = 512\n",
    "in_channels  = 1\n",
    "filters      = [32, 64, 128]\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = AutoEncoder(input_height, input_width, latent_dim, in_channels, filters).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define MultiResolutionSTFTLoss\n",
    "loss_fn = auraloss.freq.MelSTFTLoss(\n",
    "    sample_rate=sample_rate,\n",
    "    fft_size=n_fft,\n",
    "    hop_size=hop_length,\n",
    "    win_length=n_fft,\n",
    "    n_mels=n_mels\n",
    ").to(device)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50\n",
    "log_interval = 10\n",
    "avg_epoch_time = 0.0\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate based on validation loss\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "print(f\"Starting training on {device}...\")\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    model.train()\n",
    "    start_epoch_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training Loop\n",
    "    for i, (mel_spec, _, _, _) in enumerate(train_loader):\n",
    "        mel_spec = mel_spec.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spec)\n",
    "        \n",
    "        # Convert the mel-spectrogram back to waveform using InverseMelScale\n",
    "        inverse_mel = T.InverseMelScale(\n",
    "            n_stft=n_fft // 2 + 1,  # Ensure this matches n_fft from MelSpectrogram\n",
    "            mel_scale=\"slaney\"       # Matches the default Mel scale used in the forward transform\n",
    "        ).to(device)\n",
    "\n",
    "        # Convert the output mel spectrogram to waveform\n",
    "        output_waveform = inverse_mel(output)  # Shape: [batch_size, 1, time_samples]\n",
    "        print(\"output_waveform.shape = \", output_waveform.shape)\n",
    "        target_waveform = inverse_mel(mel_spec)  # Shape: [batch_size, 1, time_samples]\n",
    "        print(\"target_waveform.shape = \", target_waveform.shape)\n",
    "\n",
    "        # Squeeze batch and channel dimensions if needed\n",
    "        output_waveform = output_waveform.squeeze(1)  # Shape: [batch_size, time_samples]\n",
    "        print(\"output_waveform.shape = \", output_waveform.shape)\n",
    "        target_waveform = target_waveform.squeeze(1)  # Shape: [batch_size, time_samples]\n",
    "        print(\"target_waveform.shape = \", target_waveform.shape)\n",
    "\n",
    "        # Calculate the loss between the reconstructed waveform and the target waveform\n",
    "        loss = loss_fn(output_waveform, target_waveform)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * mel_spec.size(0)\n",
    "\n",
    "    # Compute average epoch loss for training\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (mel_spec, _, _, _) in enumerate(valid_loader):\n",
    "            mel_spec = mel_spec.to(device)\n",
    "            output = model(mel_spec)\n",
    "            \n",
    "            output_waveform = inverse_mel(output)  # Convert to waveform\n",
    "            \n",
    "            loss = loss_fn(output_waveform, mel_spec)  # Use MultiResolutionSTFTLoss here\n",
    "            valid_loss += loss.item() * mel_spec.size(0)\n",
    "\n",
    "    # Compute average validation loss\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    \n",
    "    # Step the scheduler with validation loss\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    epoch_time = time.time() - start_epoch_time\n",
    "    avg_epoch_time += epoch_time\n",
    "    print(f\"Epoch {epoch+1}, train_loss={train_loss}, valid_loss={valid_loss}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Save training results after the last epoch\n",
    "    if epoch == num_epochs - 1:\n",
    "        save_training_results({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"avg_epoch_time\": avg_epoch_time / num_epochs,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"training_subset_size\": training_subset_size,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"n_fft\": n_fft,\n",
    "            \"hop_length\": hop_length,\n",
    "            \"n_mels\": n_mels,\n",
    "            \"input_height\": input_height,\n",
    "            \"input_width\": input_width,\n",
    "            \"latent_dim\": latent_dim,\n",
    "            \"in_channels\": in_channels,\n",
    "            \"filters\": filters,\n",
    "        })\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --------------------------\n",
    "# Testing, Inversion, and Audio Playback\n",
    "# --------------------------\n",
    "model.eval()\n",
    "\n",
    "# Samples to compare\n",
    "test_indices = [random.choice(range(len(test_dataset))) for _ in range(10)]\n",
    "\n",
    "for idx in test_indices:\n",
    "    print(f\"\\n=== Test sample index: {idx} ===\")\n",
    "    # (mel_spec, sample_rate, key, metadata) from dataset\n",
    "    mel_spec, sample_rate, key, metadata = test_dataset[idx]\n",
    "\n",
    "    # Listen to the Original Audio with no transform applied\n",
    "    raw_waveform, raw_sr = load_raw_waveform(\"testing\", key)\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"Original audio:\")\n",
    "    display(Audio(raw_waveform.numpy(), rate=raw_sr))\n",
    "\n",
    "    # Reconstruct using the model\n",
    "    mel_spec = mel_spec.unsqueeze(0).to(device)  # shape [1, 1, n_mels, time_frames]\n",
    "    with torch.no_grad():\n",
    "        reconstructed_mel = model(mel_spec)  # shape [1, 1, n_mels, time_frames]\n",
    "\n",
    "    # Convert the reconstructed mel to waveform\n",
    "    recon_np = reconstructed_mel.squeeze().cpu().numpy()  # [n_mels, time_frames]\n",
    "    recon_power = librosa.db_to_power(recon_np)  # dB -> power\n",
    "    reconstructed_audio = librosa.feature.inverse.mel_to_audio(\n",
    "        recon_power, sr=sample_rate, n_fft=n_fft, hop_length=hop_length\n",
    "    )\n",
    "\n",
    "    print(\"Reconstructed audio:\")\n",
    "    display(Audio(reconstructed_audio, rate=raw_sr))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-info-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
